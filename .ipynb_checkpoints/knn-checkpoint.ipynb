{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy.linalg import inv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all data with the NaN values removed\n",
    "data_path = \"./hwk2_datasets/\"\n",
    "DS1m0 = np.genfromtxt(data_path + 'DS1_m_0.txt', delimiter=',')\n",
    "DS1m0 = DS1m0[np.logical_not(np.isnan(DS1m0))]\n",
    "DS1m1 = np.genfromtxt(data_path + 'DS1_m_1.txt', delimiter=',')\n",
    "DS1m1 = DS1m1[np.logical_not(np.isnan(DS1m1))]\n",
    "DS1cov = np.genfromtxt(data_path + 'DS1_Cov.txt', delimiter=',')\n",
    "DS1cov = DS1cov[:, ~np.isnan(DS1cov).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 20 features and 2000 data points for each class using multivariate Gaussian Distribution\n",
    "neg = np.random.multivariate_normal(DS1m0, DS1cov, 2000)\n",
    "neg = np.c_[neg, np.zeros(2000)]\n",
    "pos = np.random.multivariate_normal(DS1m1, DS1cov, 2000)\n",
    "pos = np.c_[pos, np.ones(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag:\n",
    "    # Save the generated data in 3 files: Train, Valid and Test\n",
    "    np.random.shuffle(neg)\n",
    "    np.random.shuffle(pos)\n",
    "\n",
    "    DS1 = np.r_[neg, pos]\n",
    "    np.savetxt(\"DS1.txt\", DS1, delimiter=\",\")\n",
    "\n",
    "    neg_test = neg[0:400, :].copy()\n",
    "    pos_test = pos[0:400, :].copy()\n",
    "    DS1_test = np.r_[neg_test, pos_test]\n",
    "    np.savetxt(\"DS1-test.txt\", DS1_test, delimiter=\",\")\n",
    "\n",
    "    neg_valid = neg[400:800, :].copy()\n",
    "    pos_valid = pos[400:800, :].copy()\n",
    "    DS1_valid = np.r_[neg_valid, pos_valid]\n",
    "    np.savetxt(\"DS1-valid.txt\", DS1_valid, delimiter=\",\")\n",
    "\n",
    "    neg_train = neg[800:, :].copy()\n",
    "    pos_train = pos[800:, :].copy()\n",
    "    DS1_train = np.r_[neg_train, pos_train]\n",
    "    np.savetxt(\"DS1-train.txt\", DS1_train, delimiter=\",\")\n",
    "else:\n",
    "    DS1_train = np.genfromtxt('DS1-train.txt', delimiter=',')\n",
    "    DS1_valid = np.genfromtxt('DS1-valid.txt', delimiter=',')\n",
    "    DS1_test = np.genfromtxt('DS1-test.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = DS1_train[:,:-1]\n",
    "Y_train = DS1_train[:,-1]\n",
    "X_valid = DS1_valid[:,:-1]\n",
    "Y_valid = DS1_valid[:,-1]\n",
    "X_test = DS1_test[:,:-1]\n",
    "Y_test = DS1_test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the parameters using Maximum Likelihood Approach\n",
    "m = len(Y_train)\n",
    "countpos = np.count_nonzero(Y_train == 1)\n",
    "countneg = np.count_nonzero(Y_train == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obviously, pi should be 0.5\n",
    "pi = 1/m*countpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_neg = pi\n",
    "p_pos = 1 - pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means\n",
    "sum_pos = np.zeros(20)\n",
    "sum_neg = np.zeros(20)\n",
    "for i in range(m):\n",
    "    xi = X_train[i, :]\n",
    "    yi = Y_train[i]\n",
    "    if yi == 1:\n",
    "        sum_pos += xi\n",
    "    else:\n",
    "        sum_neg += xi\n",
    "u1 = sum_pos/countpos\n",
    "u2 = sum_neg/countneg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance matrix\n",
    "sqr_sum = np.zeros((20,20))\n",
    "for i in range(m):\n",
    "    xi = X_train[i,:]\n",
    "    yi = Y_train[i]\n",
    "    if yi == 1:\n",
    "        sqr = np.matmul(np.transpose(np.matrix(xi-u1)), np.matrix(xi-u1))\n",
    "    else:\n",
    "        sqr = np.matmul(np.transpose(np.matrix(xi-u2)), np.matrix(xi-u2))\n",
    "    sqr_sum += sqr\n",
    "covariance = 1/m*sqr_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_neg: 0.5\n",
      "p_pos: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"p_neg: \" + str(p_neg))\n",
    "print(\"p_pos: \" + str(p_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_pos = np.zeros(len(X))\n",
    "# p_neg = np.zeros(len(X))\n",
    "# constant = (1/(2*0.5)**(20//2)*np.sqrt(np.linalg.det(covariance)))\n",
    "# for i in range(len(X)):\n",
    "#     new_pos[i] = constant * np.exp(-0.5*np.matmul(np.matmul(np.matrix(X[i]-u1),inv(covariance)),np.transpose(np.matrix(X[i]-u1))))\n",
    "#     new_neg[i] = constant * np.exp(-0.5*np.matmul(np.matmul(np.matrix(X[i]-u2),inv(covariance)),np.transpose(np.matrix(X[i]-u2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.matmul(inv(covariance), (u1-u2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = - 0.5*np.matmul(np.matmul(np.matrix(u1),inv(covariance)),np.transpose(np.matrix(u1))) + 0.5*np.matmul(np.matmul(np.matrix(u2),inv(covariance)),np.transpose(np.matrix(u2))) + math.log(p_pos/p_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.matmul(np.matrix(w), np.transpose(np.matrix(X_test))) + w0\n",
    "p = 1/(1+np.exp(-1*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(X_test, Y_test, p):\n",
    "    TP = FP = FN = TN = 0\n",
    "    for i in range(len(X_test)):\n",
    "        if p[i] > 0.5 and Y_test[i] == 1:\n",
    "            TP += 1\n",
    "        elif p[i] > 0.5 and Y_test[i] == 0:\n",
    "            FP += 1\n",
    "        elif Y_test[i] == 1:\n",
    "            FN += 1\n",
    "        else:\n",
    "            TN += 1\n",
    "    return TP, FP, FN, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 17, 18, 383)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP, FP, FN, TN = confusion_matrix(X_test, Y_test, np.array(p)[0])\n",
    "TP, FP, FN, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(TP, FP, FN, TN, verbose):\n",
    "    # Accuracy\n",
    "    accuracy = (TP+TN)/(TP + FP + FN + TN)\n",
    "    # Precision\n",
    "    precision = TP/(TP+FP)\n",
    "    # Recall\n",
    "    recall = TP/(TP+FN)\n",
    "    # F1-Measure\n",
    "    f1_measure = 2*precision*recall/(precision+recall)\n",
    "    if verbose:\n",
    "        print(\"Accuracy: \" + str(accuracy))\n",
    "        print(\"Precision: \" + str(precision))\n",
    "        print(\"Recall: \" + str(recall))\n",
    "        print(\"F1 measure: \" + str(f1_measure))\n",
    "    return accuracy, precision, recall, f1_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95625\n",
      "Precision: 0.9573934837092731\n",
      "Recall: 0.955\n",
      "F1 measure: 0.9561952440550688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.95625, 0.9573934837092731, 0.955, 0.9561952440550688)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics(TP, FP, FN, TN, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-27.09227938]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14.21457456,   8.45500745,   5.74905882,   3.21505482,\n",
       "         9.85249095,   4.28098114, -16.88250203,  23.62044423,\n",
       "        28.82678891,  -8.99539215,  12.85565493,  12.35432555,\n",
       "       -15.47284558, -12.81774405,   5.47438987, -12.82450377,\n",
       "       -29.34858007,   6.56022395,   0.83330701,   4.85822129])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "### 3.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(test, k):\n",
    "    n = len(X_train)\n",
    "    y_hat = 0\n",
    "    distances = np.zeros((n, 2))\n",
    "    for i in range(n):\n",
    "        dist = math.sqrt(np.sum((test - X_train[i]) ** 2))\n",
    "        distances[i] = [i, dist]\n",
    "    distances = distances[distances[:,1].argsort()]\n",
    "    for i in range(k):\n",
    "        y_hat += Y_train[int(distances[i][0])]\n",
    "    return y_hat/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [ 0.5425      0.54066986  0.565       0.55256724]\n",
      "5 [ 0.5675      0.56617647  0.5775      0.57178218]\n",
      "10 [ 0.5575      0.571875    0.4575      0.50833333]\n",
      "15 [ 0.5725      0.5721393   0.575       0.57356608]\n",
      "20 [ 0.56875     0.57534247  0.525       0.54901961]\n",
      "30 [ 0.575       0.58064516  0.54        0.55958549]\n",
      "40 [ 0.575       0.578125    0.555       0.56632653]\n",
      "45 [ 0.58375     0.57919622  0.6125      0.59538275]\n",
      "50 [ 0.59        0.58955224  0.5925      0.59102244]\n",
      "60 [ 0.61125     0.61152882  0.61        0.61076345]\n",
      "Wall time: 15min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k_trial = [1, 5, 10, 15, 20, 30, 40, 45, 50, 60]\n",
    "n = len(k_trial)\n",
    "m = len(X_valid)\n",
    "predict_Y = np.zeros((m, n))\n",
    "metr = np.zeros((n, 4))\n",
    "for j in range(n):\n",
    "    for i in range(m):\n",
    "        predict_Y[i, j] = KNN(X_valid[i], k_trial[j])\n",
    "    TP, FP, FN, TN = confusion_matrix(X_valid, Y_valid, predict_Y[:, j])\n",
    "    metr[j] = metrics(TP, FP, FN, TN, False)\n",
    "    print(k_trial[j], metr[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = k_trial[metr[:,3].argmax(axis=0)]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 178 190 222\n",
      "Accuracy: 0.54\n",
      "Precision: 0.5412371134020618\n",
      "Recall: 0.525\n",
      "F1 measure: 0.532994923857868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.54, 0.5412371134020618, 0.525, 0.532994923857868)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = len(X_test)\n",
    "predict = np.zeros(m)\n",
    "for i in range(m):\n",
    "    predict[i] = KNN(X_test[i], k)\n",
    "TP, FP, FN, TN = confusion_matrix(X_test, Y_test, predict)\n",
    "print(TP, FP, FN, TN)\n",
    "metrics(TP, FP, FN, TN, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all data with the NaN values removed\n",
    "DS2_c1_m1 = np.genfromtxt(data_path + 'DS2_c1_m1.txt', delimiter=',')\n",
    "DS2_c1_m1 = DS2_c1_m1[np.logical_not(np.isnan(DS2_c1_m1))]\n",
    "DS2_c1_m2 = np.genfromtxt(data_path + 'DS2_c1_m2.txt', delimiter=',')\n",
    "DS2_c1_m2 = DS2_c1_m2[np.logical_not(np.isnan(DS2_c1_m2))]\n",
    "DS2_c1_m3 = np.genfromtxt(data_path + 'DS2_c1_m3.txt', delimiter=',')\n",
    "DS2_c1_m3 = DS2_c1_m3[np.logical_not(np.isnan(DS2_c1_m3))]\n",
    "\n",
    "DS2_c2_m1 = np.genfromtxt(data_path + 'DS2_c2_m1.txt', delimiter=',')\n",
    "DS2_c2_m1 = DS2_c2_m1[np.logical_not(np.isnan(DS2_c2_m1))]\n",
    "DS2_c2_m2 = np.genfromtxt(data_path + 'DS2_c2_m2.txt', delimiter=',')\n",
    "DS2_c2_m2 = DS2_c2_m2[np.logical_not(np.isnan(DS2_c2_m2))]\n",
    "DS2_c2_m3 = np.genfromtxt(data_path + 'DS2_c2_m3.txt', delimiter=',')\n",
    "DS2_c2_m3 = DS2_c2_m3[np.logical_not(np.isnan(DS2_c2_m3))]\n",
    "\n",
    "DS2_cov1 = np.genfromtxt(data_path + 'DS2_Cov1.txt', delimiter=',')\n",
    "DS2_cov1 = DS2_cov1[:, ~np.isnan(DS2_cov1).any(axis=0)]\n",
    "DS2_cov2 = np.genfromtxt(data_path + 'DS2_Cov2.txt', delimiter=',')\n",
    "DS2_cov2 = DS2_cov2[:, ~np.isnan(DS2_cov2).any(axis=0)]\n",
    "DS2_cov3 = np.genfromtxt(data_path + 'DS2_Cov3.txt', delimiter=',')\n",
    "DS2_cov3 = DS2_cov3[:, ~np.isnan(DS2_cov3).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 20 features and 2000 data points for each class using multivariate Gaussian Distribution\n",
    "pos = 0.1*np.random.multivariate_normal(DS2_c1_m1, DS2_cov1, 2000) + 0.42*np.random.multivariate_normal(DS2_c1_m2, DS2_cov2, 2000) + 0.48*np.random.multivariate_normal(DS2_c1_m3, DS2_cov3, 2000)\n",
    "pos = np.c_[pos, np.ones(2000)]\n",
    "neg = 0.1*np.random.multivariate_normal(DS2_c2_m1, DS2_cov1, 2000) + 0.42*np.random.multivariate_normal(DS2_c2_m2, DS2_cov2, 2000) + 0.48*np.random.multivariate_normal(DS2_c2_m3, DS2_cov3, 2000)\n",
    "neg = np.c_[neg, np.zeros(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag:\n",
    "    # Save the generated data in 3 files: Train, Valid and Test\n",
    "    np.random.shuffle(neg)\n",
    "    np.random.shuffle(pos)\n",
    "\n",
    "    DS2 = np.r_[neg, pos]\n",
    "    np.savetxt(\"DS2.txt\", DS1, delimiter=\",\")\n",
    "\n",
    "    neg_test = neg[0:400, :].copy()\n",
    "    pos_test = pos[0:400, :].copy()\n",
    "    DS2_test = np.r_[neg_test, pos_test]\n",
    "    np.savetxt(\"DS2-test.txt\", DS2_test, delimiter=\",\")\n",
    "\n",
    "    neg_valid = neg[400:800, :].copy()\n",
    "    pos_valid = pos[400:800, :].copy()\n",
    "    DS2_valid = np.r_[neg_valid, pos_valid]\n",
    "    np.savetxt(\"DS2-valid.txt\", DS2_valid, delimiter=\",\")\n",
    "\n",
    "    neg_train = neg[800:, :].copy()\n",
    "    pos_train = pos[800:, :].copy()\n",
    "    DS2_train = np.r_[neg_train, pos_train]\n",
    "    np.savetxt(\"DS2-train.txt\", DS2_train, delimiter=\",\")\n",
    "else:\n",
    "    DS2_train = np.genfromtxt('DS2-train.txt', delimiter=',')\n",
    "    DS2_valid = np.genfromtxt('DS2-valid.txt', delimiter=',')\n",
    "    DS2_test = np.genfromtxt('DS2-test.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = DS2_train[:,:-1]\n",
    "Y_train = DS2_train[:,-1]\n",
    "X_valid = DS2_valid[:,:-1]\n",
    "Y_valid = DS2_valid[:,-1]\n",
    "X_test = DS2_test[:,:-1]\n",
    "Y_test = DS2_test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the parameters using Maximum Likelihood Approach\n",
    "m = len(Y_train)\n",
    "countpos = np.count_nonzero(Y_train == 1)\n",
    "countneg = np.count_nonzero(Y_train == 0)\n",
    "#obviously, pi should be 0.5\n",
    "pi = 1/m*countpos\n",
    "p_neg = pi\n",
    "p_pos = 1 - pi\n",
    "# Means\n",
    "sum_pos = np.zeros(20)\n",
    "sum_neg = np.zeros(20)\n",
    "for i in range(m):\n",
    "    xi = X_train[i, :]\n",
    "    yi = Y_train[i]\n",
    "    if yi == 1:\n",
    "        sum_pos += xi\n",
    "    else:\n",
    "        sum_neg += xi\n",
    "u1 = sum_pos/countpos\n",
    "u2 = sum_neg/countneg\n",
    "# Covariance matrix\n",
    "sqr_sum = np.zeros((20,20))\n",
    "for i in range(m):\n",
    "    xi = X_train[i,:]\n",
    "    yi = Y_train[i]\n",
    "    if yi == 1:\n",
    "        sqr = np.matmul(np.transpose(np.matrix(xi-u1)), np.matrix(xi-u1))\n",
    "    else:\n",
    "        sqr = np.matmul(np.transpose(np.matrix(xi-u2)), np.matrix(xi-u2))\n",
    "    sqr_sum += sqr\n",
    "covariance = 1/m*sqr_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_neg: 0.5\n",
      "p_pos: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"p_neg: \" + str(p_neg))\n",
    "print(\"p_pos: \" + str(p_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207, 189, 193, 211)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.matmul(inv(covariance), (u1-u2))\n",
    "w0 = - 0.5*np.matmul(np.matmul(np.matrix(u1),inv(covariance)),np.transpose(np.matrix(u1))) + 0.5*np.matmul(np.matmul(np.matrix(u2),inv(covariance)),np.transpose(np.matrix(u2))) + math.log(p_pos/p_neg)\n",
    "a = np.matmul(np.matrix(w), np.transpose(np.matrix(X_test))) + w0\n",
    "p = 1/(1+np.exp(-1*a))\n",
    "TP, FP, FN, TN = confusion_matrix(X_test, Y_test, np.array(p)[0])\n",
    "TP, FP, FN, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5225\n",
      "Precision: 0.5227272727272727\n",
      "Recall: 0.5175\n",
      "F1 measure: 0.5201005025125628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5225, 0.5227272727272727, 0.5175, 0.5201005025125628)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics(TP, FP, FN, TN, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.1908273]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04373192,  0.0538412 , -0.11376275,  0.04012931, -0.05404199,\n",
       "       -0.0747224 ,  0.12351706, -0.03953628, -0.02423614, -0.00339175,\n",
       "       -0.01211246, -0.05570324, -0.09249441,  0.03980019,  0.09719713,\n",
       "       -0.05347596, -0.08514464, -0.00760087,  0.0722346 ,  0.06802808])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [ 0.4975      0.49760766  0.52        0.50855746]\n",
      "5 [ 0.4575      0.45933014  0.48        0.46943765]\n",
      "10 [ 0.5025      0.50304878  0.4125      0.4532967 ]\n",
      "15 [ 0.51125     0.51053864  0.545       0.52720677]\n",
      "20 [ 0.515       0.51639344  0.4725      0.49347258]\n",
      "30 [ 0.5225      0.52406417  0.49        0.50645995]\n",
      "40 [ 0.53125     0.53180662  0.5225      0.52711223]\n",
      "45 [ 0.5225      0.52054795  0.57        0.54415274]\n",
      "50 [ 0.5475      0.5479798   0.5425      0.54522613]\n",
      "60 [ 0.53625     0.53598015  0.54        0.53798257]\n",
      "Wall time: 16min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k_trial = [1, 5, 10, 15, 20, 30, 40, 45, 50, 60]\n",
    "n = len(k_trial)\n",
    "m = len(X_valid)\n",
    "predict_Y = np.zeros((m, n))\n",
    "metr = np.zeros((n, 4))\n",
    "for j in range(n):\n",
    "    for i in range(m):\n",
    "        predict_Y[i, j] = KNN(X_valid[i], k_trial[j])\n",
    "    TP, FP, FN, TN = confusion_matrix(X_valid, Y_valid, predict_Y[:, j])\n",
    "    metr[j] = metrics(TP, FP, FN, TN, False)\n",
    "    print(k_trial[j], metr[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = k_trial[metr[:,3].argmax(axis=0)]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 183 176 217\n",
      "Accuracy: 0.55125\n",
      "Precision: 0.5503685503685504\n",
      "Recall: 0.56\n",
      "F1 measure: 0.5551425030978934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.55125, 0.5503685503685504, 0.56, 0.5551425030978934)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = len(X_test)\n",
    "predict = np.zeros(m)\n",
    "for i in range(m):\n",
    "    predict[i] = KNN(X_test[i], k)\n",
    "TP, FP, FN, TN = confusion_matrix(X_test, Y_test, predict)\n",
    "print(TP, FP, FN, TN)\n",
    "metrics(TP, FP, FN, TN, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
